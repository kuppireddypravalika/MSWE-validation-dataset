#!/usr/bin/env python3
"""Generate analysis_qna.json for benchmarks listed in a file.

This script loads potential analysis questions from existing code comparison
reports and asks them about each benchmark's original code. The answers are
written to ``llm_outputs/guided_optimized_code/<model>/<benchmark>/analysis_qna.json``.
"""

import argparse
import json
import ast
import re
from pathlib import Path
import sys

REPO_ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(REPO_ROOT))

from llm_integration.llm_api_utils import call_llm, ContextLengthError
from scripts.utilities import ensure_dir, read_text, write_text


def get_default_model(perf_path: Path = Path("reports/benchmark_performance.json")) -> str:
    """Return the model name recorded in benchmark_performance.json."""
    if not perf_path.exists():
        raise SystemExit(f"{perf_path} not found")

    with perf_path.open() as f:
        data = json.load(f)

    model = data.get("model")
    if not model:
        raise SystemExit(f"No model specified in {perf_path}")
    return model


def _collect_questions(bench_id: str) -> list[str]:
    """Return analysis questions for ``bench_id`` from report files."""
    base = REPO_ROOT / "reports" / "code_comparisons"
    questions: list[str] = []
    if not base.is_dir():
        return questions

    def _extract_question(obj) -> str | None:
        if isinstance(obj, dict):
            return str(obj.get("question")) if "question" in obj else None
        if isinstance(obj, str):
            text = obj.strip()
            try:
                parsed = json.loads(text)
            except Exception:
                try:
                    parsed = ast.literal_eval(text)
                except Exception:
                    parsed = None
            if isinstance(parsed, dict) and "question" in parsed:
                return str(parsed["question"])
            return text
        return str(obj)

    for model_dir in base.iterdir():
        if not model_dir.is_dir():
            continue
        for json_path in model_dir.glob(f"benchmark_{bench_id}_*.json"):
            try:
                data = json.loads(read_text(json_path))
            except Exception:
                continue
            q = data.get("potential_analysis_tool")
            if isinstance(q, list):
                for item in q:
                    val = _extract_question(item)
                    if val:
                        questions.append(val)
            else:
                val = _extract_question(q)
                if val:
                    questions.append(val)
    seen = set()
    ordered: list[str] = []
    for q in questions:
        if q not in seen:
            ordered.append(q)
            seen.add(q)
    return ordered


def _parse_questions_response(text: str) -> list[str]:
    """Parse and return question strings from LLM ``text`` output."""
    if not text.strip():
        return []

    fenced = re.search(r"```(?:json)?\s*(\{.*?\})\s*```", text, re.DOTALL)
    if fenced:
        text = fenced.group(1)

    try:
        obj = json.loads(text)
        if isinstance(obj, dict):
            q_list = obj.get("questions") or obj.get("potential_analysis_tool")
        elif isinstance(obj, list):
            q_list = obj
        else:
            q_list = None
        if isinstance(q_list, list):
            result: list[str] = []
            for q in q_list:
                if isinstance(q, dict):
                    val = q.get("question")
                    if val:
                        result.append(str(val).strip())
                else:
                    result.append(str(q).strip())
            return [q for q in result if q]
    except Exception:
        pass

    lines = [
        re.sub(r"^[\d\-*.\s]+", "", l).strip()
        for l in text.splitlines()
        if l.strip() and not l.strip().startswith("```")
    ]
    return [l for l in lines if l]


def _generate_questions(code: str, model: str, num_questions: int = 5) -> list[str]:
    """Return a list of questions generated by the LLM for ``code``."""

    prompt = (
        "You are a performance analysis expert helping to optimize the following "
        "C++ code. Suggest "
        f"{num_questions} concise questions that a novice LLM should ask to "
        "better understand potential optimizations. Respond with JSON using the "
        "key 'questions'.\n\n"
        f"{code}"
    )

    try:
        response = call_llm(prompt, model=model)
    except ContextLengthError as exc:
        print(f"Skipping question generation: context length exceeded: {exc}")
        return []
    except Exception as exc:
        print(f"Skipping question generation: {exc}")
        return []

    return _parse_questions_response(response)

def _parse_answers_response(text: str) -> list[str]:
    """Return answer strings from an LLM ``text`` response."""
    if not text.strip():
        return []

    fenced = re.search(r"```(?:json)?\s*(\{.*?\}|\[.*?\])\s*```", text, re.DOTALL)
    if fenced:
        text = fenced.group(1)

    try:
        obj = json.loads(text)
        if isinstance(obj, dict):
            answers = obj.get("answers")
        else:
            answers = obj
        if isinstance(answers, list):
            result: list[str] = []
            for a in answers:
                if isinstance(a, dict):
                    val = a.get("answer")
                    if val is not None:
                        result.append(str(val).strip())
                else:
                    result.append(str(a).strip())
            return result
    except Exception:
        pass

    lines = [
        l.strip()
        for l in text.splitlines()
        if l.strip() and not l.strip().startswith("```")
    ]
    return lines


def _ask_questions(questions: list[str], code: str, model: str) -> list[tuple[str, str]]:
    """Ask ``questions`` about ``code`` using ``model`` in a single request."""
    if not questions:
        return []

    q_lines = [f"{idx + 1}. {str(q).strip()}" for idx, q in enumerate(questions) if str(q).strip()]
    prompt = (
        "You are a performance analysis expert. Answer the following questions "
        "about the provided C++ code. Return only JSON with the key 'answers' "
        "containing a list of answers in the same order as the questions.\n\n"
        "Questions:\n" + "\n".join(q_lines) + "\n\n" +
        "Here are the relevant files including original.cpp, harness.cpp and headers:\n" +
        code
    )

    try:
        response = call_llm(prompt, model=model)
    except ContextLengthError as exc:
        return [(q, f"[Context length exceeded: {exc}]") for q in questions]
    except Exception as exc:
        return [(q, f"[LLM error: {exc}]") for q in questions]

    answers = _parse_answers_response(response)
    pairs = []
    for q, a in zip(questions, answers):
        pairs.append((q, a))
    for q in questions[len(answers):]:
        pairs.append((q, ""))
    return pairs


def generate_qna(bench_path: Path, model: str) -> None:
    """Create analysis_qna.json for ``bench_path`` using ``model``."""
    if not bench_path.is_dir():
        print(f"Skipping {bench_path}: directory does not exist")
        return

    code_files = [
        p
        for p in bench_path.iterdir()
        if p.suffix in {".cpp", ".cc", ".c", ".hpp", ".h"}
        and p.name != "optimized.cpp"
    ]
    parts = [f"// {p.name}\n{read_text(p)}" for p in sorted(code_files)]
    combined_code = "\n\n".join(parts)

    # Store generated questions separately from model-specific outputs so they
    # can be reused across different models.
    questions_dir = Path("llm_outputs") / "guided_optimized_code" / "questions"
    ensure_dir(questions_dir)
    questions_file = questions_dir / f"{bench_path.name}.json"

    out_dir = Path("llm_outputs") / "guided_optimized_code" / model / bench_path.name
    ensure_dir(out_dir)
    questions: list[str] = []
    if questions_file.exists():
        try:
            questions = json.loads(read_text(questions_file))
            if not isinstance(questions, list):
                questions = []
        except Exception:
            questions = []

    if not questions:
        bench_id_match = re.match(r"benchmark_(\d+)", bench_path.name)
        bench_id = bench_id_match.group(1) if bench_id_match else bench_path.name
        questions = _collect_questions(bench_id)
    if not questions:
        print(f"No analysis questions found for {bench_path.name}; generating via LLM")
        questions = _generate_questions(combined_code, model)
        if questions:
            write_text(questions_file, json.dumps(questions, indent=2) + "\n")
    if not questions:
        print(f"Failed to obtain analysis questions for {bench_path.name}")
        return

    qa_pairs = _ask_questions(questions, combined_code, model)

    qa_file = out_dir / "analysis_qna.json"
    write_text(qa_file, json.dumps(qa_pairs, indent=2) + "\n")
    print(f"Saved Q&A to {qa_file}")


def main() -> None:
    parser = argparse.ArgumentParser(description="Generate analysis_qna.json for benchmarks")
    parser.add_argument("--benchmarks-file", default="vector.json", help="JSON file listing benchmarks")
    parser.add_argument("--model", default=get_default_model(), help="LLM model name")
    args = parser.parse_args()

    with open(args.benchmarks_file, "r", encoding="utf-8") as f:
        data = json.load(f)

    benches = []
    for item in data.get("benchmarks", []):
        name = item.get("name") if isinstance(item, dict) else item
        if name:
            benches.append(name)

    for bench in benches:
        bench_path = Path("benchmarks") / bench
        generate_qna(bench_path, args.model)


if __name__ == "__main__":
    main()
