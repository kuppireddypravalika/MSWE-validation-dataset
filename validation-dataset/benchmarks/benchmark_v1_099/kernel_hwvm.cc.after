#include "types.h"
#include "amd64.h"
#include "mmu.h"
#include "cpu.hh"
#include "kernel.hh"
#include "bits.hh"
#include "spinlock.hh"
#include "kalloc.hh"
#include "condvar.hh"
#include "proc.hh"
#include "vm.hh"
#include "apic.hh"
#include "kstream.hh"
#include "ipi.hh"
#include "kstats.hh"
#include "cpuid.hh"
#include "vmalloc.hh"
#include "nospec-branch.hh"
#include "cmdline.hh"
#include "uefi.hh"
#include "multiboot.hh"

using namespace std;

// This creates two sets of PDTs. The first maps the text, data, and bss
// segments while the second maps the first 4GB of the direct map. The rest of
// page table initialization will happen in initpg.
template <size_t T>
struct boot_pdt {
    constexpr boot_pdt() : pte() {
        for (u64 i = 0; i != 512 * T; ++i)
          pte[i] = (512*4096*i) | (PTE_P | PTE_W | PTE_PS);
    }
    pme_t pte[512 * T] __attribute__ ((aligned (4096)));
};
boot_pdt<1> pdtcode = boot_pdt<1>();
boot_pdt<4> pdtbase = boot_pdt<4>();

// See: https://elixir.bootlin.com/linux/v5.3.12/source/arch/x86/include/asm/tlbflush.h#L151
const u8 NUM_TLB_CONTEXTS = 6;
struct tlb_context {
  u64 asid;
  u64 tlb_gen;
};
struct tlb_state {
  tlb_context contexts[NUM_TLB_CONTEXTS];

  // The index of the next context that will be used.
  u8 next_context;

  // Whether to flush the TLB when switching to kpml4.
  bool flush_pcid0;
};
percpu<tlb_state> tlb_states;

atomic<u64> next_asid;

DEFINE_PERCPU(const page_map_cache*, cur_page_map_cache);

static bool use_invpcid __attribute__((section (".qdata"))) = true;

static const char *levelnames[] __attribute__((section (".qdata"))) = {
  "PT", "PD", "PDP", "PML4"
};

static bool pcids_enabled() {
  return (mycpu()->cr3_mask & 0xfff) != 0;
}

static u64 reload_cr3() {
  u64 cr3 = rcr3();
  lcr3(cr3);
  return cr3;
}
static u64 flush_tlb_context() {
  scoped_cli cli;

  if(secrets_mapped) {
    mycpu()->cr3_noflush = 0;
    return reload_cr3();
  } else {
    u64 cr3 = reload_cr3();
    invpcid((cr3 & 0xfff) ^ 0x1, 0, INVPCID_ONE_PCID);
    return cr3;
  }
}

// One level in an x86-64 page table, typically the top level.  Many
// of the methods of pgmap assume they are being invoked on a
// top-level PML4.
struct pgmap {
  enum {
    // Page table levels
    L_PT = 0,
    L_PD = 1,
    L_PDPT = 2,
    L_PML4 = 3,

    // By the size of an entry on that level
    L_4K = L_PT,
    L_2M = L_PD,
    L_1G = L_PDPT,
    L_512G = L_PML4,

    // Semantic names
    L_PGSIZE = L_4K,
  };

private:
  std::atomic<pme_t> e[PGSIZE / sizeof(pme_t)];

public:
  // Allocate and return a new pgmap the clones the kernel part of this pgmap,
  // and populates the quasi user-visible part.
  pgmap_pair kclone_pair() const
  {
    pgmap *pml4 = (pgmap*)kalloc("PML4-pair", PGSIZE * 2);
    if (!pml4)
      throw_bad_alloc();

    pgmap_pair pair;
    pair.kernel = &pml4[0];
    pair.user = &pml4[1];

    size_t k = PX(L_PML4, KGLOBAL);
    memset(&pair.kernel->e[0], 0, 8*k);
    memmove(&pair.kernel->e[k], &e[k], 8*(512-k));

    size_t p = PX(L_PML4, KPUBLIC);
    memset(&pair.user->e[0], 0, PGSIZE);
    pair.user->e[p].store(e[p]);

    return pair;
  }

  void free(int level, int begin, int end, bool release)
  {
    if (level != L_4K) {
      for (int i = begin; i < end; i++) {
        pme_t entry = e[i].load(memory_order_relaxed);
        if ((entry & PTE_P) && !(entry & PTE_PS))
          ((pgmap*) p2v(PTE_ADDR(entry)))->free(level - 1, 0, 512, true);
      }
    }

    if (release)
      kfree(this);
  }

  // An iterator that references the page structure entry on a fixed
  // level of the page structure tree for some virtual address.
  // Moving the iterator changes the virtual address, but not the
  // level.
  class iterator
  {
    struct pgmap *pml4;
    uintptr_t va;

    // The target pgmap level of this iterator.
    int level;

    // The actual level resolve() was able to reach.  If <tt>reached
    // > level<tt> then @c cur will be null.  If <tt>reached ==
    // level</tt>, then @c cur will be non-null.
    int reached;

    // The pgmap containing @c va on level @c level.  As long as the
    // iterator moves within this pgmap, we don't have to re-walk the
    // page structure tree.
    struct pgmap *cur;

    friend struct pgmap;

    iterator(struct pgmap *pml4, uintptr_t va, int level)
      : pml4(pml4), va(va), level(level)
    {
      resolve();
    }

    // Walk the page table structure to find @c va at @c level and set @c cur.
    // If the path to @c va does not exist, sets @c cur to nullptr.
    void resolve()
    {
      cur = pml4;
      for (reached = L_PML4; reached > level; reached--) {
        atomic<pme_t> *entryp = &cur->e[PX(reached, va)];
        pme_t entry = entryp->load(memory_order_relaxed);
        if (entry & PTE_P) {
          cur = (pgmap*) p2v(PTE_ADDR(entry));
        } else {
          cur = nullptr;
          return;
        }
      }
    }

  public:
    // Default constructor
    constexpr iterator() : pml4(nullptr), va(0), level(0), reached(0),
                           cur(nullptr) { }

    // Return the page structure level this iterator is traversing.
    int get_level() const
    {
      return level;
    }

    // Return the pgmap containing the entry returned by operator*.
    // exists() must be true.
    pgmap *get_pgmap() const
    {
      return cur;
    }

    // Return the virtual address this iterator current points to.
    uintptr_t index() const
    {
      return va;
    }

    // Return the "span" over which the entry this iterator refers to
    // will remain the same.  That is, if exists() is false, then it
    // will be false for at least [index(), index()+span()).
    // Otherwise, &*it will be the same for [index(), index()+span()).
    uintptr_t span() const
    {
      // Calculate the beginning of the next entry on level 'reached'
      uintptr_t next = (va | ((1ull << PXSHIFT(reached)) - 1)) + 1;
      return next - va;
    }

    // Create this entry if it doesn't already exist.  Any created
    // directory entries will have flags <tt>flags|PTE_P|PTE_W</tt>.
    // After this, exists() will be true (though is_set() will only be
    // set if is_set() was already true).
    iterator &create(pme_t flags)
    {
      if (!cur) {
        cur = pml4;
        for (reached = L_PML4; reached > level; reached--) {
          atomic<pme_t> *entryp = &cur->e[PX(reached, va)];
          pme_t entry = entryp->load(memory_order_relaxed);
        retry:
          if ((entry & PTE_P) && !(entry & PTE_PS)) {
            cur = (pgmap*) p2v(PTE_ADDR(entry));
          } else {
            // XXX(Austin) Could use zalloc except during really early
            // boot (really, zalloc shouldn't crash during early boot).
            pgmap *next = (pgmap*) kalloc(levelnames[reached - 1]);
              if (!next)
                throw_bad_alloc();
              memset(next, 0, sizeof *next);

            if (!atomic_compare_exchange_weak(
                  entryp, &entry, v2p(next) | flags | PTE_P | PTE_W)) {
              // The above call updated entry with the current value in
              // entryp, so retry after the entry load.
              kfree(next);
              goto retry;
            }
            cur = next;
          }
        }
      }
      return *this;
    }

    // Create this entry if it doesn't already exist.  Any created
    // directory entries will have flags <tt>flags|PTE_P|PTE_W</tt>.
    // After this, exists() will be true (though is_set() will only be
    // set if is_set() was already true).
    iterator &create(pme_t flags, vmap* vmap, pgmap* user_pgmap)
    {
      if (!cur) {
        cur = pml4;
        for (reached = L_PML4; reached > level; reached--) {
          atomic<pme_t> *entryp = &cur->e[PX(reached, va)];
          pme_t entry = entryp->load(memory_order_relaxed);
        retry:
          if ((entry & PTE_P) && !(entry & PTE_PS)) {
            cur = (pgmap*) p2v(PTE_ADDR(entry));
          } else {
            bool used_qalloc = true;
            pgmap *next = (pgmap*) vmap->qalloc(levelnames[reached - 1], true);
            if (!next) {
              used_qalloc = false;
              next = (pgmap*) zalloc(levelnames[reached - 1]);
            }
            if (!next) {
              throw_bad_alloc();
            }

            if (!atomic_compare_exchange_weak(
                  entryp, &entry, v2p(next) | flags | PTE_P | PTE_W)) {
              // The above call updated entry with the current value in
              // entryp, so retry after the entry load.
              if (used_qalloc)
                vmap->qfree(next);
              else
                zfree(next);
              goto retry;
            }

            if (!used_qalloc) {
              *user_pgmap->find((u64)next).create(0, vmap, user_pgmap) = v2p(next) | PTE_P | PTE_W | PTE_NX;
            }
            cur = next;
          }
        }
      }
      return *this;
    }

    // Return true if this entry can be retrieved and set.  The entry
    // itself might not be marked present.
    bool exists() const
    {
      return cur;
    }

    // Return true if this entry both exists and is marked present.
    bool is_set() const
    {
      return cur && ((*this)->load(memory_order_relaxed) & PTE_P);
    }

    // Return a reference to the current page structure entry.  This
    // operation is only legal if exists() is true.
    atomic<pme_t> &operator*() const
    {
      return cur->e[PX(level, va)];
    }

    atomic<pme_t> *operator->() const
    {
      return &cur->e[PX(level, va)];
    }

    // Increment the iterator by @c x.
    iterator &operator+=(uintptr_t x)
    {
      uintptr_t prev = va;
      va += x;
      if ((va >> PXSHIFT(level + 1)) != (prev >> PXSHIFT(level + 1))) {
        // The bottom level changed.  Re-resolve.
        resolve();
      }
      return *this;
    }
  };

  // Return an iterator pointing to @c va at page structure level @c
  // level, where level 0 is the page table.
  iterator find(uintptr_t va, int level = L_PGSIZE)
  {
    return iterator(this, va, level);
  }
};

static_assert(sizeof(pgmap) == PGSIZE, "!(sizeof(pgmap) == PGSIZE)");

extern pgmap kpml4;
static atomic<uintptr_t> kvmallocpos;


void
set_cr3_mask(struct cpu *c) {
  if (cpuid::features().pcid && !cmdline_params.disable_pcid) {
    c->cr3_mask = 0xffffffff'ffffffff;
    lcr4(rcr4() | CR4_PCIDE);
  } else {
    c->cr3_mask = 0x7fffffff'fffff000;
    if (c->id == 0 && !cmdline_params.disable_pcid) {
      cprintf("WARN: PCIDs unsupported\n");
    }
  }
}

void
refresh_pcid_mask(void) {
  for (int c = 0; c < ncpu; c++) {
    set_cr3_mask(&cpus[c]);
  }
}

// Create a direct mapping starting at PA 0 to VA KBASE up to
// KBASEEND.  This augments the KCODE mapping created by the
// bootloader.  Perform per-core control register set up.
void
initpg(struct cpu *c)
{
  static bool kpml4_initialized;

  if (!kpml4_initialized) {
    kpml4_initialized = true;

    next_asid.store(1);

    int level = pgmap::L_2M;

    // Can we use 1GB mappings?
    if (cpuid::features().page1GB) {
      level = pgmap::L_1G;
    }

    // Make the text and rodata segments read only
    *kpml4.find(KTEXT, pgmap::L_2M).create(0) = v2p((void*)KTEXT) | PTE_P | PTE_PS;
    reload_cr3();

    // Create direct map region
    for (auto it = kpml4.find(KBASE, level); it.index() < KBASEEND; it += it.span()) {
      *it.create(0) = (it.index() - KBASE) | PTE_W | PTE_P | PTE_PS | PTE_NX;
    }
    assert(!kpml4.find(KBASEEND, level).is_set());

    // Memory mapped I/O region might have MTRRs to set its caching mode, so we
    // can't use 1GB pages. This also lets us set the framebuffer with write
    // combining mappings.
    for (auto it = kpml4.find(KBASE+0xc0000000UL, pgmap::L_2M); it.index() < KBASE+0x100000000UL;
         it += it.span()) {
      *it.create(0) = (it.index() - KBASE) | PTE_W | PTE_P | PTE_PS | PTE_NX;
    }

    // Write combining mappings for the framebuffer make screen updates
    // considerably faster.
    paddr framebuffer;
    u64 framebuffer_size;
    if(get_framebuffer(&framebuffer, &framebuffer_size)) {
      for (auto it = kpml4.find(KBASE+framebuffer, pgmap::L_2M); it.index() < KBASE+framebuffer+framebuffer_size;
           it += it.span()) {
        *it |= PTE_WC;
      }
    }

    // Create KVMALLOC area.  This doesn't map anything at this point;
    // it only fills in PML4 entries that can later be shared with all
    // other page tables.
    for (auto it = kpml4.find(KVMALLOC, pgmap::L_PDPT); it.index() < KVMALLOCEND;
         it += it.span()) {
      it.create(0);
      assert(!it.is_set());
    }
    kvmallocpos = KVMALLOC;

    // Create public area
    for (auto it = kpml4.find(KPUBLIC, pgmap::L_PDPT); it.index() < KPUBLICEND;
         it += it.span()) {
      it.create(0);
      assert(!it.is_set());
    }

    // Create UEFI area.
    if (multiboot.flags & MULTIBOOT2_FLAG_EFI_MMAP) {
      for (int i = 0; i < multiboot.efi_mmap_descriptor_count; i++) {
        auto d = (efi_memory_descriptor*)&multiboot.efi_mmap[multiboot.efi_mmap_descriptor_size*i];
        if (!(d->attributes & EFI_MEMORY_RUNTIME))
          continue;

        u64 perm = PTE_P;
        if (!(d->attributes & EFI_MEMORY_WB)) {
          if (d->attributes & EFI_MEMORY_WT) perm |= PTE_PWT;
          else if (d->attributes & EFI_MEMORY_UC) perm |= PTE_PCD;
          else panic("No supported cache mode for region");
        }
        if (d->attributes & EFI_MEMORY_XP) perm |= PTE_NX;
        if (!(d->attributes & EFI_MEMORY_WP)) perm |= PTE_W;

        for (int offset = 0; offset < d->npages*PGSIZE; offset += PGSIZE) {
          *kpml4.find(d->vaddr+offset).create(0) = (d->paddr + offset) | perm;
        }
      }
    }

    if (!cpuid::features().invpcid) {
      use_invpcid = false;
      cprintf("WARN: invpcid instructions unsupported\n");
    }
  }

  set_cr3_mask(c);
  c->cr3_noflush = 0;

  if (!cpuid::features().fsgsbase) {
    cprintf("WARN: wrfsbase instructions unsupported\n");
  } else {
    lcr4(rcr4() | CR4_FSGSBASE);
  }

  if (!cpuid::features().spec_ctrl) {
    cprintf("WARN: md-clear not advertised, attempting anyway\n");
  }
  if (!cpuid::features().spec_ctrl) {
    cprintf("WARN: spec-ctrl feature unavailable?!\n");
  }

  // Enable recording of last branch records (if supported).
  writemsr(MSR_INTEL_DEBUGCTL, 0x1);

  // Configure Page Attributes Table so 7 = Write Combining instead of uncached,
  // and everything else is left at default.
  // cprintf("%d: MSR_INTEL_PAT = %lx\n", (int)mycpu()->id, readmsr(MSR_INTEL_PAT));
  writemsr(MSR_INTEL_PAT, 0x0007040601070406llu);

  // Enable global pages. This has to happen on every core.
  lcr4(rcr4() | CR4_PGE);

  // Prevent kernel mode from writing to read-only pages.
  lcr0(rcr0() | CR0_WP);
}

// Clean up mappings that were only required during early boot.
void
cleanuppg(void)
{
  // Remove 1GB identity mapping
  *kpml4.find(0, pgmap::L_PML4) = 0;
  reload_cr3();
}

size_t
safe_read_hw(void *dst, uintptr_t src, size_t n)
{
  scoped_cli cli;
  struct mypgmap
  {
    pme_t e[PGSIZE / sizeof(pme_t)];
  } *pml4 = (struct mypgmap*)p2v(rcr3() & ~0xfff);
  for (size_t i = 0; i < n; ++i) {
    uintptr_t va = src + i;
    void *obj = pml4;
    int level;
    for (level = pgmap::L_PML4; ; level--) {
      pme_t entry = ((mypgmap*)obj)->e[PX(level, va)];
      if (!(entry & PTE_P))
        return i;
      obj = p2v(PTE_ADDR(entry));
      if (level == 0 || (entry & PTE_PS))
        break;
    }
    ((char*)dst)[i] = ((char*)obj)[va % (1ull << PXSHIFT(level))];
  }
  return n;
}

// Switch TSS and h/w page table to correspond to process p.
void
switchvm(vmap* from, vmap* to)
{
  if (from != to) {
    ensure_secrets();
    scoped_cli cli;

    if (from) {
      from->cache.active_cores_.atomic_reset(myid());
      // No need for a fence; worst case, we just get an extra shootdown.
    }

    if (!to) {
      // Switch directly to the base kernel page table, using PCID=0.
      u64 cr3 = v2p(&kpml4);
      if (!tlb_states->flush_pcid0) {
        cr3 |= CR3_NOFLUSH;
      }
      lcr3(cr3);
      mycpu()->ts.ist[1] = (u64)*nmistacktop - 16;
      tlb_states->flush_pcid0 = false;
    } else {
      to->cache.switch_to();
      mycpu()->ts.ist[1] = ((u64)&to->nmi_stacks[mycpu()->id + 1]) - 16;

      if (cmdline_params.spectre_v2 && cpuid::features().spec_ctrl) {
        indirect_branch_prediction_barrier();
      }
    }
  }
}

// Set up CPU's kernel segment descriptors.
// Run once at boot time on each CPU.
void
inittls(struct cpu *c)
{
  // Initialize cpu-local storage.
  writegs(KDSEG);
  writemsr(MSR_GS_BASE, (u64)&c->cpu);
  writemsr(MSR_GS_KERNBASE, (u64)&c->cpu);
  c->cpu = c;
  c->proc = nullptr;
}

// Allocate 'bytes' bytes in the KVMALLOC area, surrounded by at least
// 'guard' bytes of unmapped memory.  This memory must be freed with
// vmalloc_free.
void *
vmalloc_raw(size_t bytes, size_t guard, const char *name)
{
  if (kvmallocpos == 0)
    panic("vmalloc called before initpg");
  bytes = PGROUNDUP(bytes);
  guard = PGROUNDUP(guard);
  // Ensure there is always some guard space.  vmalloc_free depends on
  // this.
  if (guard < PGSIZE)
    guard = PGSIZE;

  uintptr_t base = guard + kvmallocpos.fetch_add(bytes + guard * 2);
  if (base + bytes + guard >= KVMALLOCEND)
    // Egads, we ran out of KVMALLOC space?!  Ideally, we would
    // recycle KVMALLOC space and perform a TLB shootdown, but other
    // things will surely break long before this does.
    panic("vmalloc: out of KVMALLOC space (recycling not implemented)");

  for (auto it = kpml4.find(base); it.index() < base + bytes; it += it.span()) {
    void *page = kalloc(name);
    if (!page)
      throw_bad_alloc();
    *it.create(0) = v2p(page) | PTE_P | PTE_W;
  }

  return (void*)base;
}

// Free vmalloc'd memory at ptr.  Note that this *lazily* unmaps this
// area from KVMALLOC space, so this area may remain effectively
// mapped until some indeterminate point in the future.
void
vmalloc_free(void *ptr)
{
  if ((uintptr_t)ptr % PGSIZE)
    panic("vmalloc_free: ptr %p is not page-aligned", ptr);
  if ((uintptr_t)ptr < KVMALLOC || (uintptr_t)ptr >= KVMALLOCEND)
    panic("vmalloc_free: ptr %p is not in KVMALLOC space", ptr);

  // Free and unmap until we reach the guard space.
  for (auto it = kpml4.find((uintptr_t)ptr); it.is_set(); it += it.span()) {
    kfree(p2v(PTE_ADDR(*it)));
    *it = 0;
  }

  // XXX Should release unused page table pages.  This requires a
  // global TLB shootdown, so we should only do it in large-ish
  // batches.
}

void
register_public_pages(void** pages, size_t count)
{
  for (size_t i = 0; i < count; i++) {
    *kpml4.find((uintptr_t)pages[i], pgmap::L_4K).create(0) =
      ((u64)pages[i] - KPUBLIC) | PTE_W | PTE_P | PTE_NX;
  }
}

void
unregister_public_pages(void** pages, size_t count)
{
  for (size_t i = 0; i < count; i++) {
    *kpml4.find((uintptr_t)pages[i], pgmap::L_4K).create(0) = 0;
  }

  run_on_all_cpus([]()
  {
    if (pcids_enabled()) {
      for (int i = 0; i < NUM_TLB_CONTEXTS; i++)
        tlb_states->contexts[i].asid = 0;
      if((flush_tlb_context() & 0xfff) != 0)
        tlb_states->flush_pcid0 = true;
    } else {
      reload_cr3();
    }
  });
}

void
register_public_range(void* start, size_t npages)
{
  for (size_t i = 0; i < npages; i++) {
    *kpml4.find((uintptr_t)start + i * PGSIZE, pgmap::L_4K).create(0) =
      ((u64)start + i * PGSIZE - KPUBLIC) | PTE_W | PTE_P | PTE_NX;
  }
}

void
unregister_public_range(void* start, size_t npages)
{
  for (size_t i = 0; i < npages; i++) {
    *kpml4.find((uintptr_t)start + i * PGSIZE, pgmap::L_4K).create(0) = 0;
  }

  run_on_all_cpus([]()
  {
    if (pcids_enabled()) {
      for (int i = 0; i < NUM_TLB_CONTEXTS; i++)
        tlb_states->contexts[i].asid = 0;
      if((flush_tlb_context() & 0xfff) != 0)
        tlb_states->flush_pcid0 = true;
    } else {
      reload_cr3();
    }
  });
}

void
tlb_shootdown::on_ipi()
{
  if (pcids_enabled())
    flush_tlb_context();
  else
    reload_cr3();
}

void
tlb_shootdown::perform() const
{
  if (start_ >= end_)
    return;

  // Ensure that cache invalidations happen before reading the tracker;
  // see also cache_tracker::track_switch_to().
  std::atomic_thread_fence(std::memory_order_acq_rel);

  bitset<NCPU> targets = cache_->active_cores_;
  {
    scoped_cli cli;
    if (targets[myid()]) {
      if (!pcids_enabled()) {
        reload_cr3();
      } else if (end_ > start_ && end_ - start_ <= 4 * PGSIZE && use_invpcid) {
        u64 pcid = rcr3() & 0xfff;
        for (uintptr_t va = start_; va < end_; va += PGSIZE) {
          invpcid(pcid, va, INVPCID_ONE_ADDR);
          invpcid(pcid ^ 0x1, va, INVPCID_ONE_ADDR);
        }
      } else {
        flush_tlb_context();
      }
      targets.reset(myid());
    }
  }

  if (targets.count() == 0)
    return;

  kstats::inc(&kstats::tlb_shootdown_count);
  kstats::inc(&kstats::tlb_shootdown_targets, targets.count());
  kstats::timer timer(&kstats::tlb_shootdown_cycles);
  for (auto i = targets.begin(); i != targets.end(); ++i) {
    lapic->send_tlbflush(&cpus[*i]);
  }
}

page_map_cache::page_map_cache(vmap* parent) :
  pml4s(kpml4.kclone_pair()), parent_(parent), asid_(next_asid++)
{
  if (!pml4s.kernel || !pml4s.user) {
    swarn.println("page_map_cache() out of memory\n");
    throw_bad_alloc();
  }

  // Ideally this should be done by vmap::alloc, but we don't currently have a
  // way to create hugepage mappings anywhere so plumbing through the logic
  // seems like more effort than it is worth.
  *(pml4s.user->find(KTEXT, pgmap::L_2M).create(0, parent_, pml4s.user))
    = v2p(qtext) | PTE_PS | PTE_P | PTE_W;

  // pml4s is private, so vmap::alloc needs us to do this.
  parent_->qinsert((void*)pml4s.user);
  parent_->qinsert((void*)pml4s.kernel);
}
page_map_cache::~page_map_cache()
{
  pml4s.kernel->free(pgmap::L_PML4, 0, PX(pgmap::L_PML4, KGLOBAL), false);

  pml4s.user->free(pgmap::L_PML4, 0, PX(pgmap::L_PML4, KGLOBAL), false);
  pml4s.user->free(pgmap::L_PML4, PX(pgmap::L_PML4, KBASE), 512, false);

  kfree(pml4s.kernel, PGSIZE * 2);
}

void
page_map_cache::insert(uintptr_t va, pme_t pte)
{
  if (pte & PTE_P)
    pte |= PTE_A | PTE_D;

  pml4s.user->find(va).create(PTE_U & pte, parent_, pml4s.user)->store(pte, memory_order_relaxed);
  if (va < KGLOBAL) {
    pml4s.kernel->find(va).create(PTE_U & pte, parent_, pml4s.user)->store(pte, memory_order_relaxed);
  }
}

void
page_map_cache::invalidate(uintptr_t start, uintptr_t len, tlb_shootdown *sd)
{
  sd->set_cache(this);
  for (auto it = pml4s.user->find(start); it.index() < start + len;
       it += it.span()) {
    if (it.is_set()) {
      it->store(0, memory_order_relaxed);
      sd->add_range(it.index(), it.index() + it.span());
    }
  }
  if (start < USERTOP) {
    for (auto it = pml4s.kernel->find(start); it.index() < start + len;
         it += it.span()) {
      if (it.is_set()) {
        it->store(0, memory_order_relaxed);
        sd->add_range(it.index(), it.index() + it.span());
      }
    }
  }
}

void
page_map_cache::switch_to() const
{
  active_cores_.atomic_set(myid());
  // Ensure that reads from the cache cannot move up before the tracker
  // update, and that the tracker update does not move down after the
  // cache reads.
  std::atomic_thread_fence(std::memory_order_acq_rel);

  bool flush_tlb = false;
  u64 new_tlb_gen = tlb_generation_;

  // Find next context to use
  int new_context = -1;
  for (int i = 0; i < NUM_TLB_CONTEXTS; i++) {
    if (tlb_states->contexts[i].asid == asid_) {
      flush_tlb = new_tlb_gen > tlb_states->contexts[i].tlb_gen;
      new_context = i;
      break;
    }
  }
  if(new_context == -1) {
    new_context = tlb_states->next_context;
    tlb_states->next_context = (new_context + 1) % NUM_TLB_CONTEXTS;
    flush_tlb = true;
  }
  tlb_states->contexts[new_context].asid = asid_;
  tlb_states->contexts[new_context].tlb_gen = new_tlb_gen;

  u64 cr3 = v2p(pml4s.kernel)
    | (new_context * 2 + 2)
    | (flush_tlb ? 0 : CR3_NOFLUSH);
  cr3 &= mycpu()->cr3_mask;

  if (pcids_enabled() && flush_tlb) {
    mycpu()->cr3_noflush = 0;
  }

  lcr3(cr3);
}
