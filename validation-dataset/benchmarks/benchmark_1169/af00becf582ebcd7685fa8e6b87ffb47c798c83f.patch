From af00becf582ebcd7685fa8e6b87ffb47c798c83f Mon Sep 17 00:00:00 2001
From: HydrogenSulfate <490868991@qq.com>
Date: Wed, 6 Mar 2024 15:35:36 +0800
Subject: [PATCH] [Prim] Optimize composite OP silu_double_grad (#62112)

* optimize composite OP silu_double_grad

* correct computation equation

* use grad_x_grad_mul_sigmoid to reduce duplicated computation
---
 .../api/composite_backward/composite_double_backward_api.h   | 5 +++--
 1 file changed, 3 insertions(+), 2 deletions(-)

diff --git a/paddle/fluid/prim/api/composite_backward/composite_double_backward_api.h b/paddle/fluid/prim/api/composite_backward/composite_double_backward_api.h
index 02bd7e29443c0b..9a1c3ec4d2112e 100644
--- a/paddle/fluid/prim/api/composite_backward/composite_double_backward_api.h
+++ b/paddle/fluid/prim/api/composite_backward/composite_double_backward_api.h
@@ -443,12 +443,13 @@ void silu_double_grad(const Tensor& x,
   auto sigmoid = 1 / (1 + exp<T>(-x));
   auto tmp1 = 1 - sigmoid;
   auto tmp2 = 1 + tmp1 * x;
+  auto grad_x_grad_mul_sigmoid = grad_x_grad * sigmoid;
   if (grad_out_grad) {
-    auto ddout = grad_x_grad * sigmoid * tmp2;
+    auto ddout = grad_x_grad_mul_sigmoid * tmp2;
     set_output<T>(ddout, grad_out_grad);
   }
   if (grad_x) {
-    auto dx = sigmoid * grad_x_grad * out_grad * (1 + (tmp2 - out)) * tmp1;
+    auto dx = grad_x_grad_mul_sigmoid * out_grad * (1 + (tmp2 - out)) * tmp1;
     set_output<T>(dx, grad_x);
   }
 }
