#!/usr/bin/env python3
"""Update code comparison reports with additional analysis fields.

This utility reads existing JSON files generated by ``compare_versions.py`` and
appends two fields by querying the LLM. The prompt now includes the LLM
generated optimized code so the model can compare it against the human
optimized implementation:

- ``potential_analysis_tool`` listing the analysis techniques a performance
  technique expert might apply when comparing the implementations.
- ``alignment_with_patch`` which scores how well the provided original and
  optimized code reflect the associated patch (0-10, ``null`` if no patch is
  available).

Any legacy fields related to analysis techniques are removed. Only files
matching ``benchmark_<id>_<trial>.json`` are processed. All other data in each
JSON file is preserved.
"""

import argparse
import json
import re
import sys
from pathlib import Path

REPO_ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(REPO_ROOT))

from llm_integration.llm_api_utils import call_llm, ContextLengthError
from scripts.utilities import read_text, write_text
from llm_integration.compare_versions import _parse_json_response, get_default_model


PROMPT_TEMPLATE = """You are a performance analysis expert assisting researchers in the performance tools community (e.g., HPCToolKit) to identify useful performance analysis methods. You will analyze optimization results for the benchmark '{bench}'.

Original implementation:
{orig}

Human expert optimized implementation:
{opt}

LLM optimized implementation:
{llm}

Patch diff (if available):
{patch_section}

Answer the following questions and return only a JSON object:

1. potential_analysis_tool: Provide a list of specific questions that a novice LLM model should ask about the original code to effectively guide optimization or enable human-expert-level code generation. Clearly justify each question's relevance.

2. alignment_with_patch: Do the original and optimized implementations accurately reflect the provided patch, or do they diverge significantly? Provide a divergence score from 0 (no divergence) to 10 (high divergence). If no patch is available, return null.

Return JSON with keys 'potential_analysis_tool' and 'alignment_with_patch'."""


def build_prompt(
    bench: str, orig: str, opt: str, llm: str, patch: str | None
) -> str:
    patch_section = f"Patch diff:\n{patch}\n\n" if patch else ""
    return PROMPT_TEMPLATE.format(
        bench=bench,
        orig=orig,
        opt=opt,
        llm=llm,
        patch_section=patch_section,
    )


def parse_filename(path: Path) -> tuple[str, str] | None:
    match = re.match(r"benchmark_(\d+)_(\d+)\.json", path.name)
    if not match:
        return None
    return match.group(1), match.group(2)


def update_file(path: Path, model: str, overwrite: bool = False) -> None:
    parsed = parse_filename(path)
    if not parsed:
        return
    bench_id, trial = parsed
    bench_dir = REPO_ROOT / "benchmarks" / f"benchmark_{bench_id}"
    orig_file = bench_dir / "original.cpp"
    opt_file = bench_dir / "optimized.cpp"
    model_name = path.parent.name
    llm_dir = (
        REPO_ROOT
        / "llm_outputs"
        / "downstream_task_1_optimized_code"
        / model_name
        / f"benchmark_{bench_id}"
    )
    llm_file = llm_dir / f"optimized_{trial}.cpp"
    if not orig_file.exists() or not opt_file.exists() or not llm_file.exists():
        print(f"Skipping {path}: source files missing")
        return
    orig_code = read_text(orig_file)
    opt_code = read_text(opt_file)
    llm_code = read_text(llm_file)
    patch_files = list(bench_dir.glob("*.patch"))
    patch_text = read_text(patch_files[0]) if patch_files else None

    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)

    if not overwrite and "potential_analysis_tool" in data and "alignment_with_patch" in data:
        print(f"Skipping {path}: fields already exist")
        return

    # Remove deprecated analysis fields if present
    for key in [
        "potential_analysis_technique",
        "potential analysis technique",
        "potential analysis tool",
    ]:
        data.pop(key, None)

    prompt = build_prompt(bench_dir.name, orig_code, opt_code, llm_code, patch_text)
    try:
        response = call_llm(prompt, model=model)
    except ContextLengthError:
        print(f"Skipping {path}: context length exceeded")
        return
    except Exception as exc:
        print(f"Skipping {path}: {exc}")
        return

    try:
        data_new = _parse_json_response(response)
    except Exception as exc:
        raise RuntimeError(f"Failed to parse LLM response for {path}: {exc}\nResponse was:\n{response}") from exc

    data["potential_analysis_tool"] = data_new.get("potential_analysis_tool")
    data["alignment_with_patch"] = data_new.get("alignment_with_patch")

    write_text(path, json.dumps(data, indent=2) + "\n")
    print(f"Updated {path}")


def main() -> None:
    parser = argparse.ArgumentParser(description="Update comparison JSON files with additional analysis fields")
    parser.add_argument(
        "--model",
        default=get_default_model(),
        help="LLM model name",
    )
    parser.add_argument(
        "--dir",
        default=Path("reports/code_comparisons"),
        type=Path,
        help="Directory containing code comparison reports",
    )
    parser.add_argument(
        "--overwrite",
        action="store_true",
        help="Regenerate fields even if they already exist",
    )
    args = parser.parse_args()

    base = args.dir
    if not base.is_dir():
        raise SystemExit(f"{base} does not exist")

    for model_dir in base.iterdir():
        if not model_dir.is_dir():
            continue
        for json_path in sorted(model_dir.glob("benchmark_*_*.json")):
            update_file(json_path, args.model, overwrite=args.overwrite)


if __name__ == "__main__":
    main()
